<html>



<head>

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>eviau</title>



    <link rel="stylesheet" type="text/css" href="../style.css" />

</head>



<body>

    <div id="container">

        <header>

            <h1><a id="titre" href="../index.html">eviau</a></h1>

        </header>



            <div id="main">

                             

<h2 id="reading-statistical-learning-theory">Reading Statistical Learning Theory</h2>

<p>Lately, I’ve been in the need for a better understanding of the theoretical grounds of machine learning algorithms. Yesterday, I finally had the opportunity to grab Vladimir N. Vapnik’s book, “Statistical Learning Theory”.</p>

<p>Here are some interesting excerpts. If I have the time, I will do my best to write a full blog post (later) on things I learned in the book. Please note that I am no expert. Let’s explore !</p>

<p>If you wish to discuss any of the following excerpts, please get in touch.</p>

<ul>

<li>“We show that if uniform two-sided convergence does not take place, then the method of empirical risk minimization is nonfalsifiable.” (p. 108.)</li>

</ul>

<p>It is interesting to note that there are thereoretical results pertaining to the notion of falsiability.</p>

<p>(more to come)</p>

        

        <nav role="navigation"><a href="../extenso.html">vers extenso // to extenso </a></nav>

        </div>



        <footer>

            dernière mise-à-jour: 2 janvier 2021, Montréal QC (Canada).

        </footer>

    </div>



</body>



</html>

